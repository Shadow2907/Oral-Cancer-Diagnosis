import os
import json
import csv
import requests
import hashlib
import re
from bs4 import BeautifulSoup
from PIL import Image
import io

API_KEY = "AIzaSyAXigAFsUQKJ0vkyrGlCxmz_43l0XUyUlQ"  # Thay b·∫±ng API Key c·ªßa b·∫°n
CX = "954606781420b4598"  # Thay b·∫±ng Custom Search Engine ID
CACHE_FILE = "cache.json"
IMAGE_JSON_FILE = "image_data.json"  # File l∆∞u th√¥ng tin ·∫£nh

class Scraper:
    def __init__(self, search_query, num_results=10, output_dir="output"):
        self.search_query = search_query
        self.num_results = num_results
        self.output_dir = output_dir
        self.urls = []
        self.cache = self.load_cache()
        self.image_data = self.load_image_data()

        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(f"{self.output_dir}/images", exist_ok=True)

    def load_cache(self):
        """T·∫£i cache t·ª´ file JSON"""
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def save_cache(self):
        """L∆∞u cache v√†o file JSON"""
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=4)

    def load_image_data(self):
        """T·∫£i d·ªØ li·ªáu ·∫£nh t·ª´ file JSON"""
        if os.path.exists(IMAGE_JSON_FILE):
            with open(IMAGE_JSON_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        return []

    def save_image_data(self):
        """L∆∞u d·ªØ li·ªáu ·∫£nh v√†o file JSON"""
        with open(IMAGE_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(self.image_data, f, ensure_ascii=False, indent=4)

    def get_search_results(self):
        """L·∫•y danh s√°ch URL t·ª´ Google Custom Search API v√† b·ªè qua URL ƒë√£ c√≥ trong cache."""
        print(f"üîç ƒêang t√¨m ki·∫øm: {self.search_query}")

        new_urls = []
        start = 1  # B·∫Øt ƒë·∫ßu t·ª´ trang ƒë·∫ßu ti√™n

        while len(new_urls) < self.num_results:
            url = (
                f"https://www.googleapis.com/customsearch/v1"
                f"?q={self.search_query}&key={API_KEY}&cx={CX}"
                f"&num=10&start={start}"
            )

            try:
                response = requests.get(url)
                data = response.json()
                urls = [item['link'] for item in data.get("items", [])]

                # L·ªçc URL ch∆∞a c√≥ trong cache
                for u in urls:
                    if u not in self.cache:
                        new_urls.append(u)
                    if len(new_urls) >= self.num_results:
                        break  # ƒê·ªß s·ªë l∆∞·ª£ng c·∫ßn crawl

                start += 10  # L·∫•y trang ti·∫øp theo n·∫øu c·∫ßn

                if not urls:  # N·∫øu API kh√¥ng tr·∫£ v·ªÅ th√™m k·∫øt qu·∫£, d·ª´ng v√≤ng l·∫∑p
                    break

            except Exception as e:
                print(f"‚ùå L·ªói khi g·ªçi API: {e}")
                break

        self.urls = new_urls
        return self.urls

    def scrape_page(self, url):
        """L·∫•y d·ªØ li·ªáu t·ª´ trang web n·∫øu ch∆∞a c√≥ trong cache"""
        if url in self.cache:
            print(f"‚úÖ ƒê√£ c√≥ trong cache: {url}")
            return self.cache[url]

        try:
            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(response.text, "html.parser")

            title = soup.title.text if soup.title else "No title"
            paragraphs = [p.text.strip() for p in soup.find_all("p") if p.text.strip()]
            content = "\n".join(paragraphs)

            page_data = {"url": url, "title": title, "content": content}
            self.cache[url] = page_data
            self.save_cache()
            return page_data

        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y d·ªØ li·ªáu t·ª´ {url}: {e}")
            return None

    def is_medical_imaging(self, img_url, alt_text, filename):
        """Ki·ªÉm tra xem ·∫£nh c√≥ ph·∫£i l√† h√¨nh ·∫£nh y t·∫ø nh∆∞ CT, MRI, X-ray kh√¥ng"""
        # Ki·ªÉm tra URL v√† alt text
        medical_terms = ['ct scan', 'ct-scan', 'x-ray', 'xray', 'x ray', 'mri', 'ultrasound', 'radiograph', 
                          'radiology', 'medical imaging', 'magnetic resonance', 'scan result']
        
        url_lower = img_url.lower()
        alt_lower = alt_text.lower() if alt_text else ""
        
        for term in medical_terms:
            if term in url_lower or term in alt_lower:
                return True
        
        try:
            # M·ªü ·∫£nh ƒë·ªÉ ph√¢n t√≠ch th√™m
            img = Image.open(filename)
            # ·∫¢nh X-ray, CT, MRI th∆∞·ªùng c√≥ m√†u x√°m/ƒëen tr·∫Øng v√† ƒë·ªô t∆∞∆°ng ph·∫£n cao
            if img.mode == 'L':  # Grayscale image
                return True
                
            # Ki·ªÉm tra th√™m ƒë·∫∑c ƒëi·ªÉm c·ªßa ·∫£nh y t·∫ø
            if img.mode in ('RGB', 'RGBA'):
                # Convert to grayscale
                gray = img.convert('L')
                histogram = gray.histogram()
                
                # T√≠nh to√°n ƒë·ªô t∆∞∆°ng ph·∫£n
                min_val = min(i for i, count in enumerate(histogram) if count > 0)
                max_val = max(i for i, count in enumerate(histogram) if count > 0)
                contrast = max_val - min_val
                
                # H√¨nh ·∫£nh y t·∫ø th∆∞·ªùng c√≥ ƒë·ªô t∆∞∆°ng ph·∫£n cao v√† ph√¢n b·ªë tone m√†u ƒë·∫∑c tr∆∞ng
                if contrast > 200 and sum(histogram[0:50]) > sum(histogram[200:256]):
                    return True
        except Exception:
            pass  # N·∫øu kh√¥ng m·ªü ƒë∆∞·ª£c ·∫£nh, b·ªè qua ph·∫ßn n√†y
            
        return False

    def is_logo_icon_or_meme(self, img_url, width, height, alt_text):
        """Ki·ªÉm tra xem ·∫£nh c√≥ ph·∫£i l√† logo, icon ho·∫∑c meme kh√¥ng"""
        # Logo v√† icon th∆∞·ªùng c√≥ k√≠ch th∆∞·ªõc nh·ªè ho·∫∑c vu√¥ng
        if width and height:
            if width < 100 or height < 100:
                return True
            
            # Logo th∆∞·ªùng c√≥ t·ª∑ l·ªá g·∫ßn vu√¥ng
            aspect_ratio = width / height if height != 0 else 0
            if 0.8 <= aspect_ratio <= 1.2:
                # C√≥ kh·∫£ nƒÉng l√† logo
                pass
        
        # Ki·ªÉm tra URL v√† alt text
        url_lower = img_url.lower()
        alt_lower = alt_text.lower() if alt_text else ""
        
        # C√°c t·ª´ kh√≥a g·ª£i √Ω l√† logo, icon, meme
        logo_terms = ['logo', 'icon', 'symbol', 'meme', 'button', 'badge', 'emoji']
        for term in logo_terms:
            if term in url_lower or term in alt_lower:
                return True
        
        # Ki·ªÉm tra c√°c ƒë∆∞·ªùng d·∫´n th∆∞·ªùng ch·ª©a logo/icon
        if any(x in url_lower for x in ['/logo', '/icon', '/assets', '/static', '/images/site']):
            return True
            
        return False

    def is_real_mouth_photo(self, img, alt_text):
        """Ki·ªÉm tra xem ·∫£nh c√≥ ph·∫£i l√† ·∫£nh th·∫≠t ch·ª•p mi·ªáng b·∫±ng m√°y ·∫£nh kh√¥ng"""
        try:
            if not img:
                return False
                
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc ·∫£nh - ·∫£nh th·ª±c t·∫ø th∆∞·ªùng c√≥ k√≠ch th∆∞·ªõc l·ªõn h∆°n
            width, height = img.size
            if width < 200 or height < 200:
                return False
                
            # Ki·ªÉm tra alt text c√≥ li√™n quan ƒë·∫øn mi·ªáng, m√¥i, ho·∫∑c ung th∆∞ m√¥i
            if alt_text:
                alt_lower = alt_text.lower()
                mouth_terms = ['mouth', 'lip', 'oral', 'tongue', 'teeth', 'cancer', 'sore', 'lesion', 'ulcer']
                if any(term in alt_lower for term in mouth_terms):
                    return True
                    
            # Ki·ªÉm tra ƒë·∫∑c ƒëi·ªÉm m√†u s·∫Øc ph·ªï bi·∫øn c·ªßa ·∫£nh mi·ªáng th·ª±c t·∫ø
            # ·∫¢nh m√¥i/mi·ªáng th∆∞·ªùng c√≥ t√¥ng m√†u da, ƒë·ªè, h·ªìng
            if img.mode in ('RGB', 'RGBA'):
                # L·∫•y m·∫´u pixel ƒë·ªÉ ki·ªÉm tra m√†u da, m√†u ƒë·ªè (m√¥i)
                pixels = img.resize((10, 10)).getdata()  # L·∫•y m·∫´u nh·ªè ƒë·ªÉ ph√¢n t√≠ch
                skin_tone_count = 0
                red_tone_count = 0
                
                for r, g, b, *_ in pixels:
                    # Ki·ªÉm tra m√†u da
                    if (r > 180 and g > 140 and b > 100) and (r > g > b):
                        skin_tone_count += 1
                    # Ki·ªÉm tra m√†u ƒë·ªè/h·ªìng c·ªßa m√¥i
                    if (r > 150 and g < 120 and b < 120) or (r > 200 and g > 100 and b > 100 and r > g and r > b):
                        red_tone_count += 1
                
                # N·∫øu c√≥ ƒë·ªß pixel m√†u da v√† m√†u ƒë·ªè, c√≥ kh·∫£ nƒÉng l√† ·∫£nh mi·ªáng th·ª±c
                if skin_tone_count > 20 or red_tone_count > 20:
                    return True
                    
            return False
                
        except Exception as e:
            print(f"L·ªói khi ph√¢n t√≠ch ·∫£nh: {e}")
            return False

    def analyze_image(self, img_data):
        """Ph√¢n t√≠ch ·∫£nh ƒë·ªÉ x√°c ƒë·ªãnh n√≥ l√† lo·∫°i g√¨"""
        try:
            img = Image.open(io.BytesIO(img_data))
            return img
        except Exception:
            return None

    def download_images(self, url):
        """T·∫£i ·∫£nh t·ª´ trang nh∆∞ng ch·ªâ gi·ªØ l·∫°i ·∫£nh mi·ªáng th·ª±c t·∫ø, lo·∫°i b·ªè CT/MRI/X-ray, logo, icon, meme"""
        try:
            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(response.text, "html.parser")

            img_tags = soup.find_all("img")
            img_data_list = []  # L∆∞u danh s√°ch ·∫£nh ƒë·ªÉ l∆∞u v√†o JSON
            saved_count = 0

            for img in img_tags:
                if "src" not in img.attrs:
                    continue

                img_url = img["src"]
                if img_url.startswith("//"):
                    img_url = "https:" + img_url

                if not img_url.startswith("http"):
                    continue  # B·ªè qua ·∫£nh kh√¥ng c√≥ URL ƒë·∫ßy ƒë·ªß

                # T·∫°o t√™n file b·∫±ng hash c·ªßa URL ·∫£nh ƒë·ªÉ tr√°nh tr√πng l·∫∑p
                img_hash = hashlib.md5(img_url.encode()).hexdigest()
                img_name = f"{self.output_dir}/images/{img_hash}.jpg"
                temp_name = f"{self.output_dir}/images/temp_{img_hash}.jpg"

                # L·∫•y th√¥ng tin alt v√† title
                alt_text = img.get("alt", "")
                title_text = img.get("title", "")
                description = alt_text if alt_text else title_text

                # Ki·ªÉm tra k√≠ch th∆∞·ªõc t·ª´ thu·ªôc t√≠nh
                width = int(img.get("width", 0)) if img.get("width", "").isdigit() else 0
                height = int(img.get("height", 0)) if img.get("height", "").isdigit() else 0

                # T·∫£i ·∫£nh ƒë·ªÉ ph√¢n t√≠ch
                try:
                    img_response = requests.get(img_url, timeout=5)
                    if img_response.status_code != 200:
                        continue

                    img_content = img_response.content
                    analyzed_img = self.analyze_image(img_content)
                    
                    # L·∫•y k√≠ch th∆∞·ªõc th·∫≠t n·∫øu kh√¥ng c√≥ trong thu·ªôc t√≠nh
                    if analyzed_img and (width == 0 or height == 0):
                        width, height = analyzed_img.size

                    # Lo·∫°i b·ªè ·∫£nh CT, MRI, X-ray
                    if self.is_medical_imaging(img_url, description, temp_name):
                        print(f"üî¨ B·ªè qua ·∫£nh y t·∫ø (CT/MRI/X-ray): {img_url}")
                        continue

                    # Lo·∫°i b·ªè logo, icon, meme
                    if self.is_logo_icon_or_meme(img_url, width, height, description):
                        print(f"üîÑ B·ªè qua logo/icon/meme: {img_url}")
                        continue

                    # Ki·ªÉm tra xem c√≥ ph·∫£i ·∫£nh mi·ªáng th·ª±c kh√¥ng
                    if not self.is_real_mouth_photo(analyzed_img, description):
                        print(f"‚ùå Kh√¥ng ph·∫£i ·∫£nh mi·ªáng: {img_url}")
                        continue
                    
                    # Ki·ªÉm tra n·∫øu ·∫£nh ƒë√£ t·ªìn t·∫°i th√¨ b·ªè qua
                    if os.path.exists(img_name):
                        print(f"‚ö° ·∫¢nh ƒë√£ t·ªìn t·∫°i: {img_name}")
                        continue

                    # L∆∞u ·∫£nh ph√π h·ª£p
                    with open(img_name, "wb") as f:
                        f.write(img_content)
                    
                    saved_count += 1
                    print(f"‚úÖ ƒê√£ l∆∞u ·∫£nh mi·ªáng th·ª±c: {img_name}")

                    # L∆∞u metadata
                    img_data_list.append({
                        "url": img_url,
                        "filename": img_name,
                        "label": description or self.search_query,
                        "width": width,
                        "height": height
                    })

                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω ·∫£nh {img_url}: {e}")
                    continue

            # Th√™m v√†o d·ªØ li·ªáu ·∫£nh chung
            self.image_data.extend(img_data_list)
            
            # L∆∞u metadata v√†o JSON
            with open(f"{self.output_dir}/image_metadata_{img_hash}.json", "w", encoding="utf-8") as f:
                json.dump(img_data_list, f, ensure_ascii=False, indent=4)

            print(f"üì∏ ƒê√£ t·∫£i {saved_count} ·∫£nh mi·ªáng th·ª±c t·ª´ {url}")

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi t·∫£i ·∫£nh t·ª´ {url}: {e}")

    def run(self):
        """Ch·∫°y to√†n b·ªô qu√° tr√¨nh"""
        self.get_search_results()

        print(f"üì• ƒêang thu th·∫≠p d·ªØ li·ªáu t·ª´ {len(self.urls)} trang web...")
        results = [self.scrape_page(url) for url in self.urls if url]
        results = [r for r in results if r]  # L·ªçc k·∫øt qu·∫£ h·ª£p l·ªá

        for url in self.urls:
            self.download_images(url)

        self.save_image_data()  # L∆∞u d·ªØ li·ªáu ·∫£nh v√†o file JSON
        print(f"üéØ Ho√†n th√†nh! D·ªØ li·ªáu ·∫£nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {IMAGE_JSON_FILE}")

if __name__ == "__main__":
    scraper = Scraper(search_query="Oral cavity cancer", num_results=10)
    scraper.run()
